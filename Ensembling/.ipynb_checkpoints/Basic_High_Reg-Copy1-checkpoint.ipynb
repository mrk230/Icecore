{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size x_train 1203\n",
      "size x_test 401\n",
      "size y_train 1203\n",
      "size y_test 401\n",
      "size x full 1604\n",
      "size y full 1604\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras.layers import GlobalMaxPooling2D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "training_path = \"../Test_data/train.json\"\n",
    "testing_path = \"../Test_data/test.json\"\n",
    "\n",
    "train_data = pd.read_json(training_path)\n",
    "test_data = pd.read_json(testing_path)\n",
    "\n",
    "\n",
    "# no third for inc angle\n",
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))    # , c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "X = get_scaled_imgs(train_data)\n",
    "\n",
    "\n",
    "\n",
    "Y = train_data[\"is_iceberg\"]\n",
    "\n",
    "\n",
    "\n",
    "X_full = X\n",
    "Y_full = Y\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size = 0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(\"size x_train\", len(X_train))\n",
    "print(\"size x_test\", len(X_test))\n",
    "\n",
    "print(\"size y_train\", len(y_train))\n",
    "print(\"size y_test\", len(y_test))\n",
    "\n",
    "print(\"size x full\", len(X_full))\n",
    "print(\"size y full\", len(Y_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 75, 75, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,557,633\n",
      "Trainable params: 4,556,225\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "img_input = Input(shape=(75,75,3))\n",
    "x = Convolution2D(64, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block1_conv1')(img_input)\n",
    "x = Convolution2D(64, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(128, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block2_conv1')(x)\n",
    "x = Convolution2D(128, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block3_conv1')(x)\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block3_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block4_conv1')(x)\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block4_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(512, kernel_regularizer=regularizers.l2(0.1),\n",
    "          bias_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, kernel_regularizer=regularizers.l2(0.1),\n",
    "          bias_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "main_output = Dense(1, activation='sigmoid', name='predictions')(x)\n",
    "\n",
    "model = Model(inputs=[img_input], outputs= [main_output], name='vgg_inc')\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "file_path = \"model_weights_basic_reg10_full.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "51/51 [==============================] - 11s 218ms/step - loss: 247.6931 - acc: 0.5648\n",
      "Epoch 2/160\n",
      " 1/51 [..............................] - ETA: 8s - loss: 246.4596 - acc: 0.6562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:494: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:403: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 7s 135ms/step - loss: 245.5045 - acc: 0.6060\n",
      "Epoch 3/160\n",
      "51/51 [==============================] - 7s 135ms/step - loss: 243.2222 - acc: 0.6279\n",
      "Epoch 4/160\n",
      "51/51 [==============================] - 7s 135ms/step - loss: 241.0147 - acc: 0.6434\n",
      "Epoch 5/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 238.8675 - acc: 0.6564\n",
      "Epoch 6/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 236.6734 - acc: 0.6776\n",
      "Epoch 7/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 234.4979 - acc: 0.6845\n",
      "Epoch 8/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 232.3719 - acc: 0.6990\n",
      "Epoch 9/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 230.2153 - acc: 0.7053\n",
      "Epoch 10/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 228.0471 - acc: 0.7199\n",
      "Epoch 11/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 225.9575 - acc: 0.7125\n",
      "Epoch 12/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 223.7910 - acc: 0.7200\n",
      "Epoch 13/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 221.6016 - acc: 0.7419\n",
      "Epoch 14/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 219.4518 - acc: 0.7341\n",
      "Epoch 15/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 217.2797 - acc: 0.7372\n",
      "Epoch 16/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 215.1701 - acc: 0.7554\n",
      "Epoch 17/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 213.0362 - acc: 0.7433\n",
      "Epoch 18/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 210.9037 - acc: 0.7604\n",
      "Epoch 19/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 208.7360 - acc: 0.7726\n",
      "Epoch 20/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 206.7041 - acc: 0.7476\n",
      "Epoch 21/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 204.5488 - acc: 0.7781\n",
      "Epoch 22/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 202.4066 - acc: 0.7740\n",
      "Epoch 23/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 200.3703 - acc: 0.7746\n",
      "Epoch 24/160\n",
      "51/51 [==============================] - 7s 136ms/step - loss: 198.2206 - acc: 0.7788\n",
      "Epoch 25/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 196.1573 - acc: 0.7856\n",
      "Epoch 26/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 194.1047 - acc: 0.7647\n",
      "Epoch 27/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 191.9915 - acc: 0.7911\n",
      "Epoch 28/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 189.9323 - acc: 0.7924\n",
      "Epoch 29/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 187.8711 - acc: 0.7917\n",
      "Epoch 30/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 185.8130 - acc: 0.8070\n",
      "Epoch 31/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 183.8519 - acc: 0.7936\n",
      "Epoch 32/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 181.7966 - acc: 0.7983\n",
      "Epoch 33/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 179.7593 - acc: 0.8114\n",
      "Epoch 34/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 177.7734 - acc: 0.7973\n",
      "Epoch 35/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 175.8013 - acc: 0.7906\n",
      "Epoch 36/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 173.7789 - acc: 0.8107\n",
      "Epoch 37/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 171.8525 - acc: 0.7984\n",
      "Epoch 38/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 169.8669 - acc: 0.8106\n",
      "Epoch 39/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 167.8897 - acc: 0.8155\n",
      "Epoch 40/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 165.9566 - acc: 0.8217\n",
      "Epoch 41/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 164.0536 - acc: 0.8077\n",
      "Epoch 42/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 162.1098 - acc: 0.8114\n",
      "Epoch 43/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 160.1559 - acc: 0.8308\n",
      "Epoch 44/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 158.2842 - acc: 0.8180\n",
      "Epoch 45/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 156.3625 - acc: 0.8217\n",
      "Epoch 46/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 154.4698 - acc: 0.8235\n",
      "Epoch 47/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 152.5793 - acc: 0.8314\n",
      "Epoch 48/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 150.7342 - acc: 0.8229\n",
      "Epoch 49/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 148.8574 - acc: 0.8327\n",
      "Epoch 50/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 147.0210 - acc: 0.8236\n",
      "Epoch 51/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 145.1588 - acc: 0.8204\n",
      "Epoch 52/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 143.3451 - acc: 0.8180\n",
      "Epoch 53/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 141.4788 - acc: 0.8285\n",
      "Epoch 54/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 139.6859 - acc: 0.8316\n",
      "Epoch 55/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 137.8847 - acc: 0.8321\n",
      "Epoch 56/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 136.0744 - acc: 0.8394\n",
      "Epoch 57/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 134.2905 - acc: 0.8272\n",
      "Epoch 58/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 132.5274 - acc: 0.8403\n",
      "Epoch 59/160\n",
      "51/51 [==============================] - 7s 139ms/step - loss: 130.7557 - acc: 0.8462\n",
      "Epoch 60/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 128.9949 - acc: 0.8455\n",
      "Epoch 61/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 127.2651 - acc: 0.8347\n",
      "Epoch 62/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 125.5187 - acc: 0.8437\n",
      "Epoch 63/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 123.7960 - acc: 0.8468\n",
      "Epoch 64/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 122.0858 - acc: 0.8450\n",
      "Epoch 65/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 120.3555 - acc: 0.8603\n",
      "Epoch 66/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 118.6933 - acc: 0.8480\n",
      "Epoch 67/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 116.9992 - acc: 0.8621\n",
      "Epoch 68/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 115.3368 - acc: 0.8493\n",
      "Epoch 69/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 113.6609 - acc: 0.8554\n",
      "Epoch 70/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 112.0114 - acc: 0.8524\n",
      "Epoch 71/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 110.3708 - acc: 0.8486\n",
      "Epoch 72/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 108.7481 - acc: 0.8481\n",
      "Epoch 73/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 107.1021 - acc: 0.8635\n",
      "Epoch 74/160\n",
      "51/51 [==============================] - 7s 139ms/step - loss: 105.5152 - acc: 0.8457\n",
      "Epoch 75/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 103.9000 - acc: 0.8590\n",
      "Epoch 76/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 102.3293 - acc: 0.8609\n",
      "Epoch 77/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 100.7532 - acc: 0.8719\n",
      "Epoch 78/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 99.2072 - acc: 0.8542\n",
      "Epoch 79/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 97.6639 - acc: 0.8586\n",
      "Epoch 80/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 96.1447 - acc: 0.8397\n",
      "Epoch 81/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 94.6087 - acc: 0.8603\n",
      "Epoch 82/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 93.0900 - acc: 0.8737\n",
      "Epoch 83/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 91.6026 - acc: 0.8688\n",
      "Epoch 84/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 90.1325 - acc: 0.8719\n",
      "Epoch 85/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 7s 138ms/step - loss: 88.6975 - acc: 0.8495\n",
      "Epoch 86/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 87.2116 - acc: 0.8842\n",
      "Epoch 87/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 85.7854 - acc: 0.8737\n",
      "Epoch 88/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 84.3435 - acc: 0.8829\n",
      "Epoch 89/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 82.9342 - acc: 0.8708\n",
      "Epoch 90/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 81.5226 - acc: 0.8860\n",
      "Epoch 91/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 80.1319 - acc: 0.8806\n",
      "Epoch 92/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 78.7686 - acc: 0.8689\n",
      "Epoch 93/160\n",
      "51/51 [==============================] - 7s 139ms/step - loss: 77.4057 - acc: 0.8593\n",
      "Epoch 94/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 76.0337 - acc: 0.8848\n",
      "Epoch 95/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 74.7025 - acc: 0.8842\n",
      "Epoch 96/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 73.3626 - acc: 0.8921\n",
      "Epoch 97/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 72.0714 - acc: 0.8719\n",
      "Epoch 98/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 70.7928 - acc: 0.8689\n",
      "Epoch 99/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 69.4852 - acc: 0.8702\n",
      "Epoch 100/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 68.2132 - acc: 0.8817\n",
      "Epoch 101/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 66.9845 - acc: 0.8725\n",
      "Epoch 102/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 65.7506 - acc: 0.8727\n",
      "Epoch 103/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 64.5034 - acc: 0.8812\n",
      "Epoch 104/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 63.3060 - acc: 0.8734\n",
      "Epoch 105/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 62.1394 - acc: 0.8636\n",
      "Epoch 106/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 60.9562 - acc: 0.8813\n",
      "Epoch 107/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 59.8074 - acc: 0.8787\n",
      "Epoch 108/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 58.6976 - acc: 0.8726\n",
      "Epoch 109/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 57.5502 - acc: 0.8879\n",
      "Epoch 110/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 56.4592 - acc: 0.8786\n",
      "Epoch 111/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 55.3617 - acc: 0.8860\n",
      "Epoch 112/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 54.3099 - acc: 0.8831\n",
      "Epoch 113/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 53.2427 - acc: 0.8689\n",
      "Epoch 114/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 52.1998 - acc: 0.8910\n",
      "Epoch 115/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 51.1490 - acc: 0.8873\n",
      "Epoch 116/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 50.1478 - acc: 0.8861\n",
      "Epoch 117/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 49.1249 - acc: 0.8959\n",
      "Epoch 118/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 48.1433 - acc: 0.8921\n",
      "Epoch 119/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 47.1688 - acc: 0.8916\n",
      "Epoch 120/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 46.2216 - acc: 0.8898\n",
      "Epoch 121/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 45.2721 - acc: 0.8891\n",
      "Epoch 122/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 44.3185 - acc: 0.9056\n",
      "Epoch 123/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 43.4191 - acc: 0.8854\n",
      "Epoch 124/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 42.5275 - acc: 0.8861\n",
      "Epoch 125/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 41.6625 - acc: 0.8825\n",
      "Epoch 126/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 40.7786 - acc: 0.8904\n",
      "Epoch 127/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 39.9039 - acc: 0.9087\n",
      "Epoch 128/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 39.0650 - acc: 0.8983\n",
      "Epoch 129/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 38.2513 - acc: 0.8916\n",
      "Epoch 130/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 37.4450 - acc: 0.8897\n",
      "Epoch 131/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 36.6371 - acc: 0.8909\n",
      "Epoch 132/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 35.8318 - acc: 0.9002\n",
      "Epoch 133/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 35.0728 - acc: 0.8996\n",
      "Epoch 134/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 34.3184 - acc: 0.9014\n",
      "Epoch 135/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 33.5698 - acc: 0.9032\n",
      "Epoch 136/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 32.8593 - acc: 0.8927\n",
      "Epoch 137/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 32.1375 - acc: 0.9013\n",
      "Epoch 138/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 31.4520 - acc: 0.8842\n",
      "Epoch 139/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 30.7437 - acc: 0.8983\n",
      "Epoch 140/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 30.0605 - acc: 0.9087\n",
      "Epoch 141/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 29.4062 - acc: 0.8970\n",
      "Epoch 142/160\n",
      "51/51 [==============================] - 7s 139ms/step - loss: 28.7644 - acc: 0.8983\n",
      "Epoch 143/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 28.1113 - acc: 0.9105\n",
      "Epoch 144/160\n",
      "51/51 [==============================] - 7s 139ms/step - loss: 27.4733 - acc: 0.9148\n",
      "Epoch 145/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 26.9141 - acc: 0.8868\n",
      "Epoch 146/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 26.2813 - acc: 0.9056\n",
      "Epoch 147/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 25.7211 - acc: 0.8904\n",
      "Epoch 148/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 25.1545 - acc: 0.9044\n",
      "Epoch 149/160\n",
      "51/51 [==============================] - 7s 139ms/step - loss: 24.6146 - acc: 0.8830\n",
      "Epoch 150/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 24.0713 - acc: 0.8959\n",
      "Epoch 151/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 23.5138 - acc: 0.9020\n",
      "Epoch 152/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 22.9929 - acc: 0.9124\n",
      "Epoch 153/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 22.4836 - acc: 0.9185\n",
      "Epoch 154/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 21.9962 - acc: 0.9106\n",
      "Epoch 155/160\n",
      "51/51 [==============================] - 7s 138ms/step - loss: 21.5134 - acc: 0.9081\n",
      "Epoch 156/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 21.0546 - acc: 0.9045\n",
      "Epoch 157/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 20.5917 - acc: 0.9019\n",
      "Epoch 158/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 20.1737 - acc: 0.9009\n",
      "Epoch 159/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 19.7182 - acc: 0.9057\n",
      "Epoch 160/160\n",
      "51/51 [==============================] - 7s 137ms/step - loss: 19.2900 - acc: 0.9020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e33425978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training code\n",
    "#%% Image data augmentation \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,                      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,                  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,                 # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,                   # randomly flip images\n",
    "    vertical_flip=False,                    # randomly flip images\n",
    "    zoom_range=0.2)                     \n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit_generator(datagen.flow(X_full, Y_full, shuffle=True,\n",
    "                    batch_size=32), epochs = 160,\n",
    "                     verbose= 1, callbacks=callbacks) # validation_data = (X_test, y_test),\n",
    "                        #callbacks=callbacks)   # validation_data = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if doing no val set (i.e. final) only\n",
    "filepath_full = 'model_weights_inc_full_reg10_Fin.hdf5'\n",
    "model.save(filepath_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604/1604 [==============================] - 3s 2ms/step\n",
      "Test loss: 19.0173571437\n",
      "Test accuracy: 0.927057356608\n",
      "len of pred test 8424\n",
      "len of id 8424\n"
     ]
    }
   ],
   "source": [
    "# from above, need to see the amount of epochs (80 +) that val stops increasing at, then do \n",
    "# that in one full go and save the model\n",
    "filepath_full = 'model_weights_inc_full_reg10_Fin.hdf5'\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "# if not using val\n",
    "file_path_use = filepath_full\n",
    "# if using val\n",
    "#file_path_use = file_path\n",
    "inf_model = load_model(file_path_use)\n",
    "\n",
    "score = inf_model.evaluate(X_full, Y_full, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "X_sub = get_scaled_imgs(test_data)\n",
    "\n",
    "predicted_test=inf_model.predict(X_sub)\n",
    "\n",
    "print(\"len of pred test\", len(predicted_test))\n",
    "print(\"len of id\", len(test_data['id']))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_data['id']\n",
    "submission['is_iceberg']=predicted_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some examination and clipping here first\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "submission.to_csv('sub_full_basic_reg01_Fin.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
