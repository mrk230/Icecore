{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size x_train 1158\n",
      "size x_test 386\n",
      "size y_train 1158\n",
      "size y_test 386\n",
      "size x full 1544\n",
      "size y full 1544\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras.layers import GlobalMaxPooling2D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "import random\n",
    "\n",
    "# get list of hold out data for ensemble training\n",
    "random.seed(17332)\n",
    "nums = random.sample(range(0,1507),60)\n",
    "\n",
    "training_path = \"../../Test_data/train.json\"\n",
    "testing_path = \"../../Test_data/test.json\"\n",
    "\n",
    "train_data_whole = pd.read_json(training_path)\n",
    "train_data = train_data_whole.loc[~train_data_whole.index.isin(list(nums))]\n",
    "\n",
    "test_data = pd.read_json(testing_path)\n",
    "\n",
    "\n",
    "# no third for inc angle\n",
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))    # , c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "X = get_scaled_imgs(train_data)\n",
    "\n",
    "\n",
    "\n",
    "Y = train_data[\"is_iceberg\"]\n",
    "\n",
    "\n",
    "\n",
    "X_full = X\n",
    "Y_full = Y\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size = 0.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(\"size x_train\", len(X_train))\n",
    "print(\"size x_test\", len(X_test))\n",
    "\n",
    "print(\"size y_train\", len(y_train))\n",
    "print(\"size y_test\", len(y_test))\n",
    "\n",
    "print(\"size x full\", len(X_full))\n",
    "print(\"size y full\", len(Y_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75, 75, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,557,633\n",
      "Trainable params: 4,556,225\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "img_input = Input(shape=(75,75,3))\n",
    "x = Convolution2D(64, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block1_conv1')(img_input)\n",
    "x = Convolution2D(64, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(128, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block2_conv1')(x)\n",
    "x = Convolution2D(128, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block3_conv1')(x)\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block3_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block4_conv1')(x)\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block4_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(512, kernel_regularizer=regularizers.l2(0.1),\n",
    "          bias_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, kernel_regularizer=regularizers.l2(0.1),\n",
    "          bias_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "main_output = Dense(1, activation='sigmoid', name='predictions')(x)\n",
    "\n",
    "model = Model(inputs=[img_input], outputs= [main_output], name='vgg_inc')\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "file_path = \"model_weights_basic_reg10_ensem.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "49/49 [==============================] - 13s 269ms/step - loss: 247.9362 - acc: 0.5542\n",
      "Epoch 2/150\n",
      " 1/49 [..............................] - ETA: 6s - loss: 247.1866 - acc: 0.5312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:494: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py:403: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 7s 134ms/step - loss: 245.9437 - acc: 0.5917\n",
      "Epoch 3/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 243.9568 - acc: 0.5976\n",
      "Epoch 4/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 241.8573 - acc: 0.6377\n",
      "Epoch 5/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 239.8573 - acc: 0.6353\n",
      "Epoch 6/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 237.9020 - acc: 0.6575\n",
      "Epoch 7/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 235.8486 - acc: 0.6785\n",
      "Epoch 8/150\n",
      "49/49 [==============================] - 7s 137ms/step - loss: 233.9592 - acc: 0.6627\n",
      "Epoch 9/150\n",
      "49/49 [==============================] - 7s 136ms/step - loss: 232.0210 - acc: 0.6761\n",
      "Epoch 10/150\n",
      "49/49 [==============================] - 7s 136ms/step - loss: 229.9876 - acc: 0.6932\n",
      "Epoch 11/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 227.9783 - acc: 0.7015\n",
      "Epoch 12/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 226.0324 - acc: 0.7181\n",
      "Epoch 13/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 224.0369 - acc: 0.7175\n",
      "Epoch 14/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 222.0863 - acc: 0.7111\n",
      "Epoch 15/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 220.1243 - acc: 0.7296\n",
      "Epoch 16/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 218.1824 - acc: 0.7437\n",
      "Epoch 17/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 216.2282 - acc: 0.7379\n",
      "Epoch 18/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 214.2846 - acc: 0.7417\n",
      "Epoch 19/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 212.3194 - acc: 0.7398\n",
      "Epoch 20/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 210.3466 - acc: 0.7449\n",
      "Epoch 21/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 208.4948 - acc: 0.7430\n",
      "Epoch 22/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 206.5435 - acc: 0.7538\n",
      "Epoch 23/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 204.6373 - acc: 0.7615\n",
      "Epoch 24/150\n",
      "49/49 [==============================] - 7s 140ms/step - loss: 202.7181 - acc: 0.7481\n",
      "Epoch 25/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 200.7690 - acc: 0.7596\n",
      "Epoch 26/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 198.8328 - acc: 0.7698\n",
      "Epoch 27/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 196.9212 - acc: 0.7761\n",
      "Epoch 28/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 195.0521 - acc: 0.7812\n",
      "Epoch 29/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 193.1305 - acc: 0.7749\n",
      "Epoch 30/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 191.2717 - acc: 0.7768\n",
      "Epoch 31/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 189.3934 - acc: 0.7762\n",
      "Epoch 32/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 187.4797 - acc: 0.7921\n",
      "Epoch 33/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 185.6361 - acc: 0.7749\n",
      "Epoch 34/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 183.6997 - acc: 0.7979\n",
      "Epoch 35/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 181.8673 - acc: 0.7940\n",
      "Epoch 36/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 180.0678 - acc: 0.7813\n",
      "Epoch 37/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 178.1671 - acc: 0.7992\n",
      "Epoch 38/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 176.2850 - acc: 0.8150\n",
      "Epoch 39/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 174.4603 - acc: 0.8099\n",
      "Epoch 40/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 172.6591 - acc: 0.7985\n",
      "Epoch 41/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 170.8163 - acc: 0.8113\n",
      "Epoch 42/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 168.9987 - acc: 0.8106\n",
      "Epoch 43/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 167.1603 - acc: 0.8182\n",
      "Epoch 44/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 165.3830 - acc: 0.8081\n",
      "Epoch 45/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 163.5516 - acc: 0.8176\n",
      "Epoch 46/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 161.7504 - acc: 0.8183\n",
      "Epoch 47/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 159.9688 - acc: 0.8246\n",
      "Epoch 48/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 158.2124 - acc: 0.8189\n",
      "Epoch 49/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 156.4296 - acc: 0.8081\n",
      "Epoch 50/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 154.6539 - acc: 0.8209\n",
      "Epoch 51/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 152.8542 - acc: 0.8374\n",
      "Epoch 52/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 151.1383 - acc: 0.8259\n",
      "Epoch 53/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 149.3333 - acc: 0.8476\n",
      "Epoch 54/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 147.6198 - acc: 0.8399\n",
      "Epoch 55/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 145.8897 - acc: 0.8202\n",
      "Epoch 56/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 144.1577 - acc: 0.8304\n",
      "Epoch 57/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 142.4461 - acc: 0.8406\n",
      "Epoch 58/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 140.7548 - acc: 0.8157\n",
      "Epoch 59/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 139.0326 - acc: 0.8342\n",
      "Epoch 60/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 137.3075 - acc: 0.8380\n",
      "Epoch 61/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 135.6501 - acc: 0.8374\n",
      "Epoch 62/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 133.9151 - acc: 0.8501\n",
      "Epoch 63/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 132.2666 - acc: 0.8412\n",
      "Epoch 64/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 130.5920 - acc: 0.8522\n",
      "Epoch 65/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 128.9124 - acc: 0.8533\n",
      "Epoch 66/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 127.3016 - acc: 0.8380\n",
      "Epoch 67/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 125.6327 - acc: 0.8558\n",
      "Epoch 68/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 123.9804 - acc: 0.8539\n",
      "Epoch 69/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 122.3518 - acc: 0.8571\n",
      "Epoch 70/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 120.7275 - acc: 0.8571\n",
      "Epoch 71/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 119.1415 - acc: 0.8450\n",
      "Epoch 72/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 117.5249 - acc: 0.8526\n",
      "Epoch 73/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 115.9018 - acc: 0.8591\n",
      "Epoch 74/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 114.3478 - acc: 0.8502\n",
      "Epoch 75/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 112.7584 - acc: 0.8502\n",
      "Epoch 76/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 111.1942 - acc: 0.8495\n",
      "Epoch 77/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 109.6310 - acc: 0.8597\n",
      "Epoch 78/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 108.0950 - acc: 0.8514\n",
      "Epoch 79/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 106.5498 - acc: 0.8623\n",
      "Epoch 80/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 105.0202 - acc: 0.8566\n",
      "Epoch 81/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 103.4893 - acc: 0.8547\n",
      "Epoch 82/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 101.9916 - acc: 0.8591\n",
      "Epoch 83/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 100.4844 - acc: 0.8654\n",
      "Epoch 84/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 98.9996 - acc: 0.8699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 97.5158 - acc: 0.8667\n",
      "Epoch 86/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 96.0776 - acc: 0.8598\n",
      "Epoch 87/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 94.6026 - acc: 0.8711\n",
      "Epoch 88/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 93.1645 - acc: 0.8705\n",
      "Epoch 89/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 91.7354 - acc: 0.8617\n",
      "Epoch 90/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 90.3038 - acc: 0.8757\n",
      "Epoch 91/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 88.8893 - acc: 0.8616\n",
      "Epoch 92/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 87.4758 - acc: 0.8846\n",
      "Epoch 93/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 86.1051 - acc: 0.8603\n",
      "Epoch 94/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 84.7166 - acc: 0.8706\n",
      "Epoch 95/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 83.3540 - acc: 0.8750\n",
      "Epoch 96/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 81.9998 - acc: 0.8750\n",
      "Epoch 97/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 80.6602 - acc: 0.8839\n",
      "Epoch 98/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 79.3178 - acc: 0.8794\n",
      "Epoch 99/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 77.9901 - acc: 0.8833\n",
      "Epoch 100/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 76.6997 - acc: 0.8833\n",
      "Epoch 101/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 75.4001 - acc: 0.8756\n",
      "Epoch 102/150\n",
      "49/49 [==============================] - 6s 133ms/step - loss: 74.1139 - acc: 0.8750\n",
      "Epoch 103/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 72.8489 - acc: 0.8750\n",
      "Epoch 104/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 71.5937 - acc: 0.8782\n",
      "Epoch 105/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 70.3325 - acc: 0.8871\n",
      "Epoch 106/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 69.0915 - acc: 0.8820\n",
      "Epoch 107/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 67.8754 - acc: 0.8852\n",
      "Epoch 108/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 66.6650 - acc: 0.8782\n",
      "Epoch 109/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 65.4680 - acc: 0.8814\n",
      "Epoch 110/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 64.3084 - acc: 0.8795\n",
      "Epoch 111/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 63.1199 - acc: 0.8794\n",
      "Epoch 112/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 61.9726 - acc: 0.8871\n",
      "Epoch 113/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 60.8145 - acc: 0.8801\n",
      "Epoch 114/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 59.6870 - acc: 0.8833\n",
      "Epoch 115/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 58.5729 - acc: 0.8884\n",
      "Epoch 116/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 57.4671 - acc: 0.8865\n",
      "Epoch 117/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 56.4060 - acc: 0.8827\n",
      "Epoch 118/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 55.3069 - acc: 0.8884\n",
      "Epoch 119/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 54.2500 - acc: 0.8903\n",
      "Epoch 120/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 53.2008 - acc: 0.8955\n",
      "Epoch 121/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 52.1747 - acc: 0.8858\n",
      "Epoch 122/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 51.1607 - acc: 0.8852\n",
      "Epoch 123/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 50.1772 - acc: 0.8783\n",
      "Epoch 124/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 49.1670 - acc: 0.8922\n",
      "Epoch 125/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 48.1841 - acc: 0.8941\n",
      "Epoch 126/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 47.2387 - acc: 0.8788\n",
      "Epoch 127/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 46.2778 - acc: 0.8986\n",
      "Epoch 128/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 45.3518 - acc: 0.8967\n",
      "Epoch 129/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 44.4489 - acc: 0.8872\n",
      "Epoch 130/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 43.5452 - acc: 0.8941\n",
      "Epoch 131/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 42.6539 - acc: 0.8928\n",
      "Epoch 132/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 41.7857 - acc: 0.8847\n",
      "Epoch 133/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 40.9185 - acc: 0.9018\n",
      "Epoch 134/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 40.1019 - acc: 0.8789\n",
      "Epoch 135/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 39.2641 - acc: 0.8954\n",
      "Epoch 136/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 38.4605 - acc: 0.8890\n",
      "Epoch 137/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 37.6430 - acc: 0.8935\n",
      "Epoch 138/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 36.8605 - acc: 0.8826\n",
      "Epoch 139/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 36.0778 - acc: 0.8935\n",
      "Epoch 140/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 35.3298 - acc: 0.9037\n",
      "Epoch 141/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 34.5793 - acc: 0.8941\n",
      "Epoch 142/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 33.8623 - acc: 0.8986\n",
      "Epoch 143/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 33.1146 - acc: 0.9075\n",
      "Epoch 144/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 32.4284 - acc: 0.9012\n",
      "Epoch 145/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 31.7616 - acc: 0.8890\n",
      "Epoch 146/150\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 31.0523 - acc: 0.9037\n",
      "Epoch 147/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 30.4023 - acc: 0.8942\n",
      "Epoch 148/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 29.7706 - acc: 0.8877\n",
      "Epoch 149/150\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 29.1074 - acc: 0.8999\n",
      "Epoch 150/150\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 28.4879 - acc: 0.9037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ac8613860>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training code\n",
    "#%% Image data augmentation \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,                      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,                  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,                 # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,                   # randomly flip images\n",
    "    vertical_flip=False,                    # randomly flip images\n",
    "    zoom_range=0.2)                     \n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit_generator(datagen.flow(X_full, Y_full, shuffle=True,\n",
    "                    batch_size=32), epochs = 150,\n",
    "                     verbose= 1, callbacks=callbacks) # validation_data = (X_test, y_test),\n",
    "                        #callbacks=callbacks)   # validation_data = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if doing no val set (i.e. final) only\n",
    "filepath_full = 'model_weights_basic_reg10_ensem.hdf5'\n",
    "model.save(filepath_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1544/1544 [==============================] - 2s 2ms/step\n",
      "Test loss: 28.1331595683\n",
      "Test accuracy: 0.919041450777\n",
      "len of pred test 60\n",
      "len of id 8424\n"
     ]
    }
   ],
   "source": [
    "# from above, need to see the amount of epochs (80 +) that val stops increasing at, then do \n",
    "# that in one full go and save the model\n",
    "filepath_full = 'model_weights_basic_reg10_ensem.hdf5'\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "# if not using val\n",
    "file_path_use = filepath_full\n",
    "# if using val\n",
    "#file_path_use = file_path\n",
    "inf_model = load_model(file_path_use)\n",
    "\n",
    "score = inf_model.evaluate(X_full, Y_full, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# for hold out submission to train ensemble\n",
    "hold_out_train = train_data.iloc[nums]\n",
    "\n",
    "\n",
    "X_sub = get_scaled_imgs(hold_out_train)\n",
    "\n",
    "predicted_test=inf_model.predict(X_sub)\n",
    "\n",
    "print(\"len of pred test\", len(predicted_test))\n",
    "print(\"len of id\", len(test_data['id']))\n",
    "\n",
    "submission_ensem = pd.DataFrame()\n",
    "submission_ensem['id']=hold_out_train['id']\n",
    "submission_ensem['is_iceberg']=predicted_test\n",
    "\n",
    "submission_ensem.to_csv(\"sub_full_basic_ensem.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test set submission\n",
    "X_sub = get_scaled_imgs(test_data)\n",
    "\n",
    "predicted_test=inf_model.predict(X_sub)\n",
    "\n",
    "print(\"len of pred test\", len(predicted_test))\n",
    "print(\"len of id\", len(test_data['id']))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_data['id']\n",
    "submission['is_iceberg']=predicted_test\n",
    "\n",
    "\n",
    "\n",
    "submission.to_csv('sub_full_basic_reg01_Fin.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
