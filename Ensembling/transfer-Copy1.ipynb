{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:25: FutureWarning: 'select' is deprecated and will be removed in a future release. You can use .loc[labels.map(crit)] as a replacement\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size x_train 1158\n",
      "size x_test 386\n",
      "size y_train 1158\n",
      "size y_test 386\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "import random\n",
    "\n",
    "# get list of hold out data for ensemble training\n",
    "random.seed(17332)\n",
    "nums = random.sample(range(0,1507),60)\n",
    "\n",
    "\n",
    "training_path = \"../../Test_data/train.json\"\n",
    "testing_path = \"../../Test_data/test.json\"\n",
    "\n",
    "train_data_whole = pd.read_json(training_path)\n",
    "train_data = train_data_whole.select(lambda x: x not in nums)\n",
    "\n",
    "test_data = pd.read_json(testing_path)\n",
    "\n",
    "\n",
    "# no third for inc angle\n",
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))    # , c)))\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "X_orig = get_scaled_imgs(train_data)\n",
    "\n",
    "# resize to 150x150\n",
    "X = []\n",
    "for i in X_orig:\n",
    "    X.append(cv2.resize(i, (150,150)))\n",
    "X = np.array(X)\n",
    "\n",
    "Y = train_data[\"is_iceberg\"]\n",
    "\n",
    "\n",
    "X_full = X\n",
    "Y_full = Y\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, \n",
    "                                                                         random_state=42)\n",
    "\n",
    "print(\"size x_train\", len(X_train))\n",
    "print(\"size x_test\", len(X_test))\n",
    "print(\"size y_train\", len(y_train))\n",
    "print(\"size y_test\", len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled\n"
     ]
    }
   ],
   "source": [
    "# this could also be the output a different Keras model or layer\n",
    "input_tensor = Input(shape=(150, 150, 3))  \n",
    "\n",
    "base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, kernel_regularizer=regularizers.l2(.1),\n",
    "          bias_regularizer=regularizers.l2(.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"compiled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "49/49 [==============================] - 10s 194ms/step - loss: 135.5199 - acc: 0.5256\n",
      "Epoch 2/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 131.6796 - acc: 0.5403\n",
      "Epoch 3/30\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 127.9304 - acc: 0.5650\n",
      "Epoch 4/30\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 124.2802 - acc: 0.5651\n",
      "Epoch 5/30\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 120.7237 - acc: 0.5925\n",
      "Epoch 6/30\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 117.2488 - acc: 0.6129\n",
      "Epoch 7/30\n",
      "49/49 [==============================] - 6s 130ms/step - loss: 113.8703 - acc: 0.6116\n",
      "Epoch 8/30\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 110.5543 - acc: 0.6506\n",
      "Epoch 9/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 107.3382 - acc: 0.6327\n",
      "Epoch 10/30\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 104.2237 - acc: 0.6530\n",
      "Epoch 11/30\n",
      "49/49 [==============================] - 6s 130ms/step - loss: 101.1874 - acc: 0.6422\n",
      "Epoch 12/30\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 98.1938 - acc: 0.6505\n",
      "Epoch 13/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 95.3151 - acc: 0.6429\n",
      "Epoch 14/30\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 92.4625 - acc: 0.6658\n",
      "Epoch 15/30\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 89.7146 - acc: 0.6716\n",
      "Epoch 16/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 87.0202 - acc: 0.6632\n",
      "Epoch 17/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 84.3986 - acc: 0.6747\n",
      "Epoch 18/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 81.8506 - acc: 0.6734\n",
      "Epoch 19/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 79.3624 - acc: 0.6766\n",
      "Epoch 20/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 76.9546 - acc: 0.6606\n",
      "Epoch 21/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 74.5704 - acc: 0.6709\n",
      "Epoch 22/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 72.2805 - acc: 0.6778\n",
      "Epoch 23/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 70.0386 - acc: 0.6785\n",
      "Epoch 24/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 67.8512 - acc: 0.7028\n",
      "Epoch 25/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 65.7150 - acc: 0.6875\n",
      "Epoch 26/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 63.6471 - acc: 0.6868\n",
      "Epoch 27/30\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 61.6216 - acc: 0.6944\n",
      "Epoch 28/30\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 59.6677 - acc: 0.6958\n",
      "Epoch 29/30\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 57.7336 - acc: 0.7187\n",
      "Epoch 30/30\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 55.8878 - acc: 0.7168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3de583780>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on the new data for a few epochs, just to get the top layer going\n",
    "\n",
    "# training code\n",
    "#%% Image data augmentation \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,               # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,                # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,    # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,     # divide each input by its std\n",
    "    zca_whitening=False,                    # apply ZCA whitening\n",
    "    rotation_range=10,                      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,                  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,                 # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,                   # randomly flip images\n",
    "    vertical_flip=True)                     # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit_generator(datagen.flow(X_full, Y_full, shuffle=True,\n",
    "                    batch_size=32), epochs = 30,\n",
    "                     verbose= 1) #, validation_data = (X_test, y_test))   # validation_data = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recompiled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "#for layer in model.layers[:249]:\n",
    "#   layer.trainable = False\n",
    "#for layer in model.layers[249:]:\n",
    "#   layer.trainable = True\n",
    "# training them all\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "file_path = \"model_weights_transf_incV3_ensem.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=15)\n",
    "\n",
    "print(\"recompiled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/85\n",
      "49/49 [==============================] - 32s 643ms/step - loss: 53.9636 - acc: 0.6767\n",
      "Epoch 2/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 51.8268 - acc: 0.7385\n",
      "Epoch 3/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 49.7979 - acc: 0.7545\n",
      "Epoch 4/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 47.8302 - acc: 0.7667\n",
      "Epoch 5/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 45.9096 - acc: 0.7947\n",
      "Epoch 6/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 44.0469 - acc: 0.8067\n",
      "Epoch 7/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 42.2777 - acc: 0.8010\n",
      "Epoch 8/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 40.5399 - acc: 0.8164\n",
      "Epoch 9/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 38.8710 - acc: 0.8195\n",
      "Epoch 10/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 37.2520 - acc: 0.8242\n",
      "Epoch 11/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 35.6998 - acc: 0.8304\n",
      "Epoch 12/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 34.1949 - acc: 0.8482\n",
      "Epoch 13/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 32.7338 - acc: 0.8559\n",
      "Epoch 14/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 31.3450 - acc: 0.8488\n",
      "Epoch 15/85\n",
      "49/49 [==============================] - 23s 468ms/step - loss: 30.0063 - acc: 0.8540\n",
      "Epoch 16/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 28.7064 - acc: 0.8507\n",
      "Epoch 17/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 27.4338 - acc: 0.8616\n",
      "Epoch 18/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 26.2242 - acc: 0.8718\n",
      "Epoch 19/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 25.0751 - acc: 0.8546\n",
      "Epoch 20/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 23.9363 - acc: 0.8769\n",
      "Epoch 21/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 22.8729 - acc: 0.8667\n",
      "Epoch 22/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 21.8316 - acc: 0.8731\n",
      "Epoch 23/85\n",
      "49/49 [==============================] - 23s 468ms/step - loss: 20.8077 - acc: 0.8820\n",
      "Epoch 24/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 19.8689 - acc: 0.8693\n",
      "Epoch 25/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 18.9408 - acc: 0.8776\n",
      "Epoch 26/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 18.0615 - acc: 0.8744\n",
      "Epoch 27/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 17.1945 - acc: 0.8935\n",
      "Epoch 28/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 16.3861 - acc: 0.8884\n",
      "Epoch 29/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 15.6101 - acc: 0.8903\n",
      "Epoch 30/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 14.8520 - acc: 0.8802\n",
      "Epoch 31/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 14.1263 - acc: 0.8834\n",
      "Epoch 32/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 13.4095 - acc: 0.9011\n",
      "Epoch 33/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 12.7642 - acc: 0.8941\n",
      "Epoch 34/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 12.1286 - acc: 0.8846\n",
      "Epoch 35/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 11.5154 - acc: 0.8928\n",
      "Epoch 36/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 10.9400 - acc: 0.8955\n",
      "Epoch 37/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 10.3818 - acc: 0.8948\n",
      "Epoch 38/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 9.8237 - acc: 0.9056\n",
      "Epoch 39/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 9.3276 - acc: 0.8872\n",
      "Epoch 40/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 8.8284 - acc: 0.8973\n",
      "Epoch 41/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 8.3670 - acc: 0.9005\n",
      "Epoch 42/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 7.9066 - acc: 0.9139\n",
      "Epoch 43/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 7.4988 - acc: 0.9057\n",
      "Epoch 44/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 7.0873 - acc: 0.9024\n",
      "Epoch 45/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 6.6956 - acc: 0.9082\n",
      "Epoch 46/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 6.3554 - acc: 0.8916\n",
      "Epoch 47/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 5.9672 - acc: 0.9082\n",
      "Epoch 48/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 5.6227 - acc: 0.9209\n",
      "Epoch 49/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 5.3123 - acc: 0.9133\n",
      "Epoch 50/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 4.9999 - acc: 0.9132\n",
      "Epoch 51/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 4.7216 - acc: 0.9063\n",
      "Epoch 52/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 4.4489 - acc: 0.9152\n",
      "Epoch 53/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 4.2050 - acc: 0.9018\n",
      "Epoch 54/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 3.9373 - acc: 0.9165\n",
      "Epoch 55/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 3.7155 - acc: 0.9120\n",
      "Epoch 56/85\n",
      "49/49 [==============================] - 23s 468ms/step - loss: 3.4950 - acc: 0.9101\n",
      "Epoch 57/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 3.2771 - acc: 0.9171\n",
      "Epoch 58/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 3.0615 - acc: 0.9209\n",
      "Epoch 59/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 2.8816 - acc: 0.9209\n",
      "Epoch 60/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 2.6985 - acc: 0.9241\n",
      "Epoch 61/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 2.5262 - acc: 0.9280\n",
      "Epoch 62/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 2.3721 - acc: 0.9177\n",
      "Epoch 63/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 2.2230 - acc: 0.9285\n",
      "Epoch 64/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 2.0913 - acc: 0.9279\n",
      "Epoch 65/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 1.9473 - acc: 0.9273\n",
      "Epoch 66/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 1.8277 - acc: 0.9260\n",
      "Epoch 67/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 1.7045 - acc: 0.9318\n",
      "Epoch 68/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 1.5951 - acc: 0.9234\n",
      "Epoch 69/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 1.5065 - acc: 0.9235\n",
      "Epoch 70/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 1.3987 - acc: 0.9298\n",
      "Epoch 71/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 1.3065 - acc: 0.9285\n",
      "Epoch 72/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 1.2113 - acc: 0.9317\n",
      "Epoch 73/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 1.1411 - acc: 0.9299\n",
      "Epoch 74/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 1.0656 - acc: 0.9229\n",
      "Epoch 75/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 0.9924 - acc: 0.9330\n",
      "Epoch 76/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 0.9107 - acc: 0.9394\n",
      "Epoch 77/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 0.8694 - acc: 0.9368\n",
      "Epoch 78/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 0.7954 - acc: 0.9407\n",
      "Epoch 79/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 0.7589 - acc: 0.9350\n",
      "Epoch 80/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 0.7040 - acc: 0.9407\n",
      "Epoch 81/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 0.6580 - acc: 0.9363\n",
      "Epoch 82/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 0.6142 - acc: 0.9407\n",
      "Epoch 83/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 0.5802 - acc: 0.9388\n",
      "Epoch 84/85\n",
      "49/49 [==============================] - 23s 467ms/step - loss: 0.5462 - acc: 0.9407\n",
      "Epoch 85/85\n",
      "49/49 [==============================] - 23s 466ms/step - loss: 0.4993 - acc: 0.9445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3a4277550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "# train the model on the new data for a few epochs, just to get the top layer going\n",
    "\n",
    "# training code\n",
    "#%% Image data augmentation \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,               # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,                # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,    # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,     # divide each input by its std\n",
    "    zca_whitening=False,                    # apply ZCA whitening\n",
    "    rotation_range=20,                      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,                  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,                 # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,                   # randomly flip images\n",
    "    vertical_flip=True)                     # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit_generator(datagen.flow(X_full, Y_full, shuffle=True,\n",
    "                    batch_size=32), epochs = 85,\n",
    "                     verbose= 1) #,callbacks = callbacks) # validation_data = (X_test, y_test),\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# if doing no val set (i.e. final) only\n",
    "filepath_full = 'model_weights_transfer_ensem.hdf5'\n",
    "model.save(filepath_full)\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of pred test 60\n",
      "len of id 60\n"
     ]
    }
   ],
   "source": [
    "# from above, need to see the amount of epochs (80 +) that val stops increasing at, then do \n",
    "# that in one full go and save the model\n",
    "filepath_full = 'model_weights_transfer_ensem.hdf5'\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "# if not using val\n",
    "file_path_use = filepath_full\n",
    "# if using val\n",
    "#file_path_use = file_path\n",
    "inf_model = load_model(file_path_use)\n",
    "\n",
    "#score = inf_model.evaluate(X_test, y_test, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "\n",
    "# for hold out submission to train ensemble\n",
    "hold_out_train = train_data_whole.iloc[nums]\n",
    "\n",
    "\n",
    "X_orig_test = get_scaled_imgs(hold_out_train)\n",
    "\n",
    "# resize to 150x150\n",
    "X_sub = []\n",
    "for i in X_orig_test:\n",
    "    X_sub.append(cv2.resize(i, (150,150)))\n",
    "X_sub = np.array(X_sub)\n",
    "\n",
    "\n",
    "predicted_test=inf_model.predict(X_sub)\n",
    "\n",
    "print(\"len of pred test\", len(predicted_test))\n",
    "print(\"len of id\", len(hold_out_train['id']))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=hold_out_train['id']\n",
    "submission['is_iceberg']=predicted_test\n",
    "submission.to_csv('sub_full_transfer_incV3_ensem.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
