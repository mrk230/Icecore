{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size x_train 1203\n",
      "size x_test 401\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 75, 75, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 75, 75, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 75, 75, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 37, 37, 64)        256       \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 37, 37, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 18, 18, 128)       512       \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 18, 18, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 18, 18, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               10617344  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 12,027,713\n",
      "Trainable params: 12,026,817\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "training_path = \"../Test_data/train.json\"\n",
    "testing_path = \"../Test_data/test.json\"\n",
    "\n",
    "train_data = pd.read_json(training_path)\n",
    "test_data = pd.read_json(testing_path)\n",
    "\n",
    "\n",
    "#Generate the training data\n",
    "#Create 2 bands having HH, HV and normalize both\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_data[\"band_1\"]])\n",
    "X_band_1 = (X_band_1-(-22.159))/(28.424**(1/2.0))\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_data[\"band_2\"]])\n",
    "X_band_2 = (X_band_2-(-24.953))/(20.669**(1/2.0))\n",
    "\n",
    "X = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],\n",
    "                          ((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "# X = np.concatenate([X_band_1[:,:,:,np.newaxis], X_band_2[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "Y = train_data[\"is_iceberg\"]\n",
    "# Y = np_utils.to_categorical(y, 2)\n",
    "X_full = X\n",
    "Y_full = Y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=42)\n",
    "\n",
    "print(\"size x_train\", len(X_train))\n",
    "print(\"size x_test\", len(X_test))\n",
    "\n",
    "# create the model\n",
    "img_input = Input(shape=(75,75,3))\n",
    "x = Convolution2D(64, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block1_conv1')(img_input)\n",
    "x = Convolution2D(64, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(128, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block2_conv1')(x)\n",
    "x = Convolution2D(128, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block3_conv1')(x)\n",
    "x = Convolution2D(256, (3,3), activation='relu', padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.1),\n",
    "                  bias_regularizer=regularizers.l2(0.1), name='block3_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(512, kernel_regularizer=regularizers.l2(0.1),\n",
    "          bias_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, kernel_regularizer=regularizers.l2(0.1),\n",
    "          bias_regularizer=regularizers.l2(0.1))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation='sigmoid', name='predictions')(x)\n",
    "\n",
    "model = Model(img_input, x, name='vggBase')\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def get_callbacks(filepath, patience=2):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [es, msave]\n",
    "file_path = \"model_weights_normed.hdf5\"\n",
    "callbacks = get_callbacks(filepath=file_path, patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "38/38 [==============================] - 9s 239ms/step - loss: 222.4883 - acc: 0.5890 - val_loss: 220.8050 - val_acc: 0.4888\n",
      "Epoch 2/180\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 220.3253 - acc: 0.5736 - val_loss: 219.1075 - val_acc: 0.4888\n",
      "Epoch 3/180\n",
      "38/38 [==============================] - 6s 165ms/step - loss: 218.1493 - acc: 0.5857 - val_loss: 217.6765 - val_acc: 0.4888\n",
      "Epoch 4/180\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 215.9624 - acc: 0.6063 - val_loss: 215.8793 - val_acc: 0.4888\n",
      "Epoch 5/180\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 213.7192 - acc: 0.6493 - val_loss: 213.8377 - val_acc: 0.4888\n",
      "Epoch 6/180\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 211.6591 - acc: 0.6405 - val_loss: 212.2558 - val_acc: 0.4888\n",
      "Epoch 7/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 209.4383 - acc: 0.6586 - val_loss: 210.0680 - val_acc: 0.4888\n",
      "Epoch 8/180\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 207.3967 - acc: 0.6438 - val_loss: 208.4202 - val_acc: 0.4888\n",
      "Epoch 9/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 205.2857 - acc: 0.6562 - val_loss: 205.6316 - val_acc: 0.4888\n",
      "Epoch 10/180\n",
      "38/38 [==============================] - 6s 166ms/step - loss: 203.0883 - acc: 0.6627 - val_loss: 203.4839 - val_acc: 0.4888\n",
      "Epoch 11/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 201.1730 - acc: 0.6515 - val_loss: 201.1476 - val_acc: 0.4913\n",
      "Epoch 12/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 198.9381 - acc: 0.6822 - val_loss: 199.0033 - val_acc: 0.5087\n",
      "Epoch 13/180\n",
      "38/38 [==============================] - 7s 191ms/step - loss: 196.8859 - acc: 0.6869 - val_loss: 196.7094 - val_acc: 0.5686\n",
      "Epoch 14/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 194.7969 - acc: 0.7099 - val_loss: 194.1993 - val_acc: 0.6234\n",
      "Epoch 15/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 192.7570 - acc: 0.6981 - val_loss: 192.2520 - val_acc: 0.6185\n",
      "Epoch 16/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 190.7033 - acc: 0.7044 - val_loss: 189.7241 - val_acc: 0.6833\n",
      "Epoch 17/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 188.7005 - acc: 0.7017 - val_loss: 187.6270 - val_acc: 0.7007\n",
      "Epoch 18/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 186.6831 - acc: 0.6913 - val_loss: 185.4371 - val_acc: 0.7456\n",
      "Epoch 19/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 184.6046 - acc: 0.7258 - val_loss: 183.4450 - val_acc: 0.7880\n",
      "Epoch 20/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 182.6249 - acc: 0.7253 - val_loss: 181.4239 - val_acc: 0.7781\n",
      "Epoch 21/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 180.6580 - acc: 0.7104 - val_loss: 179.4450 - val_acc: 0.7830\n",
      "Epoch 22/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 178.6601 - acc: 0.7294 - val_loss: 177.5051 - val_acc: 0.7781\n",
      "Epoch 23/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 176.7064 - acc: 0.7261 - val_loss: 175.4890 - val_acc: 0.8130\n",
      "Epoch 24/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 174.7395 - acc: 0.7428 - val_loss: 173.5043 - val_acc: 0.8180\n",
      "Epoch 25/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 172.7319 - acc: 0.7488 - val_loss: 171.6003 - val_acc: 0.8254\n",
      "Epoch 26/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 170.8252 - acc: 0.7480 - val_loss: 169.6772 - val_acc: 0.8304\n",
      "Epoch 27/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 168.8842 - acc: 0.7420 - val_loss: 167.7309 - val_acc: 0.8204\n",
      "Epoch 28/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 166.9535 - acc: 0.7497 - val_loss: 165.8832 - val_acc: 0.8130\n",
      "Epoch 29/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 165.0824 - acc: 0.7609 - val_loss: 163.9643 - val_acc: 0.8155\n",
      "Epoch 30/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 163.1892 - acc: 0.7524 - val_loss: 162.0576 - val_acc: 0.8379\n",
      "Epoch 31/180\n",
      "38/38 [==============================] - 6s 167ms/step - loss: 161.2759 - acc: 0.7628 - val_loss: 160.1990 - val_acc: 0.8429\n",
      "Epoch 32/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 159.4954 - acc: 0.7562 - val_loss: 158.3587 - val_acc: 0.8229\n",
      "Epoch 33/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 157.5762 - acc: 0.7593 - val_loss: 156.5479 - val_acc: 0.8279\n",
      "Epoch 34/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 155.7371 - acc: 0.7831 - val_loss: 154.6848 - val_acc: 0.8354\n",
      "Epoch 35/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 153.8914 - acc: 0.7721 - val_loss: 152.8492 - val_acc: 0.8354\n",
      "Epoch 36/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 152.0978 - acc: 0.7686 - val_loss: 151.0420 - val_acc: 0.8429\n",
      "Epoch 37/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 150.2656 - acc: 0.7845 - val_loss: 149.2592 - val_acc: 0.8354\n",
      "Epoch 38/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 148.5164 - acc: 0.7776 - val_loss: 147.4972 - val_acc: 0.8329\n",
      "Epoch 39/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 146.7194 - acc: 0.7782 - val_loss: 145.7234 - val_acc: 0.8454\n",
      "Epoch 40/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 144.8944 - acc: 0.8138 - val_loss: 143.9648 - val_acc: 0.8429\n",
      "Epoch 41/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 143.1467 - acc: 0.8050 - val_loss: 142.2223 - val_acc: 0.8479\n",
      "Epoch 42/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 141.4141 - acc: 0.8146 - val_loss: 140.4938 - val_acc: 0.8404\n",
      "Epoch 43/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 139.7082 - acc: 0.8007 - val_loss: 138.7597 - val_acc: 0.8529\n",
      "Epoch 44/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 137.9763 - acc: 0.8166 - val_loss: 137.0475 - val_acc: 0.8653\n",
      "Epoch 45/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 136.2848 - acc: 0.8034 - val_loss: 135.3540 - val_acc: 0.8504\n",
      "Epoch 46/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 134.5919 - acc: 0.7976 - val_loss: 133.6961 - val_acc: 0.8454\n",
      "Epoch 47/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 132.9359 - acc: 0.8067 - val_loss: 131.9936 - val_acc: 0.8579\n",
      "Epoch 48/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 131.2486 - acc: 0.8256 - val_loss: 130.3469 - val_acc: 0.8379\n",
      "Epoch 49/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 129.5744 - acc: 0.8322 - val_loss: 128.6860 - val_acc: 0.8603\n",
      "Epoch 50/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 127.9529 - acc: 0.8209 - val_loss: 127.1012 - val_acc: 0.8454\n",
      "Epoch 51/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 126.3197 - acc: 0.8196 - val_loss: 125.4840 - val_acc: 0.8404\n",
      "Epoch 52/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 124.7035 - acc: 0.8114 - val_loss: 123.8651 - val_acc: 0.8479\n",
      "Epoch 53/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 123.1219 - acc: 0.8193 - val_loss: 122.2940 - val_acc: 0.8379\n",
      "Epoch 54/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 121.5702 - acc: 0.8053 - val_loss: 120.7234 - val_acc: 0.8254\n",
      "Epoch 55/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 119.9599 - acc: 0.8347 - val_loss: 119.1192 - val_acc: 0.8678\n",
      "Epoch 56/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 118.3787 - acc: 0.8358 - val_loss: 117.6346 - val_acc: 0.8429\n",
      "Epoch 57/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 116.8643 - acc: 0.8366 - val_loss: 116.0405 - val_acc: 0.8579\n",
      "Epoch 58/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 115.3083 - acc: 0.8484 - val_loss: 114.5384 - val_acc: 0.8504\n",
      "Epoch 59/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 113.8434 - acc: 0.8188 - val_loss: 113.0461 - val_acc: 0.8579\n",
      "Epoch 60/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 6s 169ms/step - loss: 112.3239 - acc: 0.8322 - val_loss: 111.5553 - val_acc: 0.8579\n",
      "Epoch 61/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 110.8265 - acc: 0.8399 - val_loss: 110.1081 - val_acc: 0.8479\n",
      "Epoch 62/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 109.3523 - acc: 0.8484 - val_loss: 108.6089 - val_acc: 0.8454\n",
      "Epoch 63/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 107.9175 - acc: 0.8404 - val_loss: 107.1365 - val_acc: 0.8678\n",
      "Epoch 64/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 106.4260 - acc: 0.8634 - val_loss: 105.7106 - val_acc: 0.8628\n",
      "Epoch 65/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 105.0420 - acc: 0.8388 - val_loss: 104.2930 - val_acc: 0.8603\n",
      "Epoch 66/180\n",
      "38/38 [==============================] - 7s 173ms/step - loss: 103.6131 - acc: 0.8410 - val_loss: 102.8909 - val_acc: 0.8728\n",
      "Epoch 67/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 102.2029 - acc: 0.8539 - val_loss: 101.5036 - val_acc: 0.8628\n",
      "Epoch 68/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 100.8113 - acc: 0.8489 - val_loss: 100.1257 - val_acc: 0.8628\n",
      "Epoch 69/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 99.4341 - acc: 0.8506 - val_loss: 98.7578 - val_acc: 0.8653\n",
      "Epoch 70/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 98.0626 - acc: 0.8676 - val_loss: 97.4015 - val_acc: 0.8653\n",
      "Epoch 71/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 96.7419 - acc: 0.8610 - val_loss: 96.0583 - val_acc: 0.8579\n",
      "Epoch 72/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 95.3889 - acc: 0.8618 - val_loss: 94.7229 - val_acc: 0.8579\n",
      "Epoch 73/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 94.0973 - acc: 0.8640 - val_loss: 93.4165 - val_acc: 0.8753\n",
      "Epoch 74/180\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 92.7694 - acc: 0.8519 - val_loss: 92.1187 - val_acc: 0.8678\n",
      "Epoch 75/180\n",
      "38/38 [==============================] - 7s 171ms/step - loss: 91.4883 - acc: 0.8569 - val_loss: 90.8398 - val_acc: 0.8678\n",
      "Epoch 76/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 90.1965 - acc: 0.8744 - val_loss: 89.6061 - val_acc: 0.8454\n",
      "Epoch 77/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 88.9441 - acc: 0.8689 - val_loss: 88.3049 - val_acc: 0.8628\n",
      "Epoch 78/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 87.6940 - acc: 0.8629 - val_loss: 87.0909 - val_acc: 0.8603\n",
      "Epoch 79/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 86.4586 - acc: 0.8722 - val_loss: 85.8821 - val_acc: 0.8529\n",
      "Epoch 80/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 85.2334 - acc: 0.8610 - val_loss: 84.6431 - val_acc: 0.8603\n",
      "Epoch 81/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 84.0428 - acc: 0.8484 - val_loss: 83.4376 - val_acc: 0.8628\n",
      "Epoch 82/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 82.8423 - acc: 0.8632 - val_loss: 82.2535 - val_acc: 0.8554\n",
      "Epoch 83/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 81.6550 - acc: 0.8695 - val_loss: 81.0887 - val_acc: 0.8603\n",
      "Epoch 84/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 80.4871 - acc: 0.8665 - val_loss: 79.9186 - val_acc: 0.8678\n",
      "Epoch 85/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 79.3191 - acc: 0.8733 - val_loss: 78.7772 - val_acc: 0.8728\n",
      "Epoch 86/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 78.1927 - acc: 0.8673 - val_loss: 77.6788 - val_acc: 0.8504\n",
      "Epoch 87/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 77.0521 - acc: 0.8868 - val_loss: 76.5147 - val_acc: 0.8778\n",
      "Epoch 88/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 75.9553 - acc: 0.8733 - val_loss: 75.4094 - val_acc: 0.8753\n",
      "Epoch 89/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 74.8417 - acc: 0.8714 - val_loss: 74.4429 - val_acc: 0.8204\n",
      "Epoch 90/180\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 73.7930 - acc: 0.8654 - val_loss: 73.2669 - val_acc: 0.8703\n",
      "Epoch 91/180\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 72.6999 - acc: 0.8780 - val_loss: 72.2460 - val_acc: 0.8304\n",
      "Epoch 92/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 71.6357 - acc: 0.8824 - val_loss: 71.1383 - val_acc: 0.8728\n",
      "Epoch 93/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 70.5885 - acc: 0.8807 - val_loss: 70.0947 - val_acc: 0.8753\n",
      "Epoch 94/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 69.5519 - acc: 0.8829 - val_loss: 69.0727 - val_acc: 0.8703\n",
      "Epoch 95/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 68.5361 - acc: 0.8862 - val_loss: 68.0667 - val_acc: 0.8728\n",
      "Epoch 96/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 67.5227 - acc: 0.8846 - val_loss: 67.0629 - val_acc: 0.8803\n",
      "Epoch 97/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 66.5241 - acc: 0.8964 - val_loss: 66.0733 - val_acc: 0.8728\n",
      "Epoch 98/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 65.5658 - acc: 0.8717 - val_loss: 65.1037 - val_acc: 0.8653\n",
      "Epoch 99/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 64.5752 - acc: 0.8879 - val_loss: 64.1495 - val_acc: 0.8778\n",
      "Epoch 100/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 63.6368 - acc: 0.8794 - val_loss: 63.2653 - val_acc: 0.8454\n",
      "Epoch 101/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 62.6895 - acc: 0.8873 - val_loss: 62.2507 - val_acc: 0.8728\n",
      "Epoch 102/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 61.7503 - acc: 0.8824 - val_loss: 61.3358 - val_acc: 0.8728\n",
      "Epoch 103/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 60.8506 - acc: 0.8821 - val_loss: 60.4029 - val_acc: 0.8803\n",
      "Epoch 104/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 59.9451 - acc: 0.8785 - val_loss: 59.5495 - val_acc: 0.8678\n",
      "Epoch 105/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 59.0504 - acc: 0.8755 - val_loss: 58.6274 - val_acc: 0.8803\n",
      "Epoch 106/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 58.1758 - acc: 0.8807 - val_loss: 57.7656 - val_acc: 0.8803\n",
      "Epoch 107/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 57.2813 - acc: 0.8917 - val_loss: 56.9059 - val_acc: 0.8853\n",
      "Epoch 108/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 56.4260 - acc: 0.8892 - val_loss: 56.0420 - val_acc: 0.8703\n",
      "Epoch 109/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 55.5778 - acc: 0.8950 - val_loss: 55.2168 - val_acc: 0.8728\n",
      "Epoch 110/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 54.7532 - acc: 0.8881 - val_loss: 54.4383 - val_acc: 0.8479\n",
      "Epoch 111/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 53.9411 - acc: 0.8805 - val_loss: 53.5901 - val_acc: 0.8703\n",
      "Epoch 112/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 53.1127 - acc: 0.8942 - val_loss: 52.7828 - val_acc: 0.8728\n",
      "Epoch 113/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 52.3274 - acc: 0.8796 - val_loss: 51.9502 - val_acc: 0.8853\n",
      "Epoch 114/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 51.5173 - acc: 0.8928 - val_loss: 51.2124 - val_acc: 0.8504\n",
      "Epoch 115/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 50.7377 - acc: 0.8944 - val_loss: 50.4012 - val_acc: 0.8853\n",
      "Epoch 116/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 49.9716 - acc: 0.9016 - val_loss: 49.6503 - val_acc: 0.8828\n",
      "Epoch 117/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 49.2243 - acc: 0.8868 - val_loss: 48.9371 - val_acc: 0.8653\n",
      "Epoch 118/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 48.4768 - acc: 0.9049 - val_loss: 48.1865 - val_acc: 0.8579\n",
      "Epoch 119/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 47.7288 - acc: 0.9021 - val_loss: 47.4243 - val_acc: 0.8803\n",
      "Epoch 120/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 6s 168ms/step - loss: 47.0291 - acc: 0.8900 - val_loss: 46.7149 - val_acc: 0.8828\n",
      "Epoch 121/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 46.2868 - acc: 0.8955 - val_loss: 46.0018 - val_acc: 0.8828\n",
      "Epoch 122/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 45.5886 - acc: 0.9076 - val_loss: 45.3788 - val_acc: 0.8479\n",
      "Epoch 123/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 44.8912 - acc: 0.9051 - val_loss: 44.6189 - val_acc: 0.8753\n",
      "Epoch 124/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 44.2109 - acc: 0.9029 - val_loss: 43.9392 - val_acc: 0.8853\n",
      "Epoch 125/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 43.5398 - acc: 0.8972 - val_loss: 43.2663 - val_acc: 0.8828\n",
      "Epoch 126/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 42.8571 - acc: 0.9090 - val_loss: 42.5987 - val_acc: 0.8828\n",
      "Epoch 127/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 42.2246 - acc: 0.8966 - val_loss: 41.9433 - val_acc: 0.8803\n",
      "Epoch 128/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 41.5865 - acc: 0.8859 - val_loss: 41.3156 - val_acc: 0.8828\n",
      "Epoch 129/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 40.9460 - acc: 0.8972 - val_loss: 40.6872 - val_acc: 0.8828\n",
      "Epoch 130/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 40.3022 - acc: 0.8944 - val_loss: 40.0981 - val_acc: 0.8603\n",
      "Epoch 131/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 39.6834 - acc: 0.9147 - val_loss: 39.4787 - val_acc: 0.8703\n",
      "Epoch 132/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 39.0884 - acc: 0.9040 - val_loss: 38.8953 - val_acc: 0.8653\n",
      "Epoch 133/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 38.4975 - acc: 0.8942 - val_loss: 38.2809 - val_acc: 0.8703\n",
      "Epoch 134/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 37.8934 - acc: 0.9013 - val_loss: 37.6935 - val_acc: 0.8653\n",
      "Epoch 135/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 37.3029 - acc: 0.9142 - val_loss: 37.0840 - val_acc: 0.8803\n",
      "Epoch 136/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 36.7469 - acc: 0.8969 - val_loss: 36.5374 - val_acc: 0.8728\n",
      "Epoch 137/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 36.1985 - acc: 0.8911 - val_loss: 35.9920 - val_acc: 0.8753\n",
      "Epoch 138/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 35.6361 - acc: 0.8950 - val_loss: 35.4336 - val_acc: 0.8753\n",
      "Epoch 139/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 35.0775 - acc: 0.9123 - val_loss: 34.8669 - val_acc: 0.8878\n",
      "Epoch 140/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 34.5635 - acc: 0.8974 - val_loss: 34.3545 - val_acc: 0.8778\n",
      "Epoch 141/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 34.0261 - acc: 0.9070 - val_loss: 33.8391 - val_acc: 0.8778\n",
      "Epoch 142/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 33.4838 - acc: 0.9125 - val_loss: 33.3183 - val_acc: 0.8728\n",
      "Epoch 143/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 32.9887 - acc: 0.9007 - val_loss: 32.8313 - val_acc: 0.8703\n",
      "Epoch 144/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 32.5042 - acc: 0.8837 - val_loss: 32.3035 - val_acc: 0.8778\n",
      "Epoch 145/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 32.0040 - acc: 0.8950 - val_loss: 31.8177 - val_acc: 0.8903\n",
      "Epoch 146/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 31.5092 - acc: 0.8977 - val_loss: 31.3397 - val_acc: 0.8778\n",
      "Epoch 147/180\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 31.0259 - acc: 0.9038 - val_loss: 30.8439 - val_acc: 0.8878\n",
      "Epoch 148/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 30.5523 - acc: 0.8972 - val_loss: 30.3865 - val_acc: 0.8753\n",
      "Epoch 149/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 30.0838 - acc: 0.9134 - val_loss: 29.9212 - val_acc: 0.8953\n",
      "Epoch 150/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 29.6280 - acc: 0.9040 - val_loss: 29.4650 - val_acc: 0.8878\n",
      "Epoch 151/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 29.1785 - acc: 0.9070 - val_loss: 29.0457 - val_acc: 0.8628\n",
      "Epoch 152/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 28.7470 - acc: 0.9013 - val_loss: 28.5735 - val_acc: 0.8903\n",
      "Epoch 153/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 28.3049 - acc: 0.9101 - val_loss: 28.1414 - val_acc: 0.8803\n",
      "Epoch 154/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 27.8721 - acc: 0.9062 - val_loss: 27.7672 - val_acc: 0.8653\n",
      "Epoch 155/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 27.4534 - acc: 0.9090 - val_loss: 27.3362 - val_acc: 0.8653\n",
      "Epoch 156/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 27.0532 - acc: 0.9013 - val_loss: 26.8927 - val_acc: 0.8853\n",
      "Epoch 157/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 26.6310 - acc: 0.9087 - val_loss: 26.5075 - val_acc: 0.8728\n",
      "Epoch 158/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 26.2184 - acc: 0.9060 - val_loss: 26.0958 - val_acc: 0.8903\n",
      "Epoch 159/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 25.8349 - acc: 0.9095 - val_loss: 25.7165 - val_acc: 0.8853\n",
      "Epoch 160/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 25.4552 - acc: 0.9002 - val_loss: 25.3733 - val_acc: 0.8628\n",
      "Epoch 161/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 25.0684 - acc: 0.9079 - val_loss: 24.9536 - val_acc: 0.8853\n",
      "Epoch 162/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 24.6920 - acc: 0.9175 - val_loss: 24.5997 - val_acc: 0.8703\n",
      "Epoch 163/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 24.3449 - acc: 0.9027 - val_loss: 24.2278 - val_acc: 0.8803\n",
      "Epoch 164/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 23.9603 - acc: 0.9112 - val_loss: 23.8737 - val_acc: 0.8753\n",
      "Epoch 165/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 23.6072 - acc: 0.9172 - val_loss: 23.4955 - val_acc: 0.8728\n",
      "Epoch 166/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 23.2475 - acc: 0.9136 - val_loss: 23.1544 - val_acc: 0.8728\n",
      "Epoch 167/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 22.9100 - acc: 0.9079 - val_loss: 22.8028 - val_acc: 0.8903\n",
      "Epoch 168/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 22.5910 - acc: 0.9068 - val_loss: 22.5534 - val_acc: 0.8454\n",
      "Epoch 169/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 22.2532 - acc: 0.9060 - val_loss: 22.1665 - val_acc: 0.8678\n",
      "Epoch 170/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 21.9064 - acc: 0.9183 - val_loss: 21.8262 - val_acc: 0.8803\n",
      "Epoch 171/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 21.6112 - acc: 0.8879 - val_loss: 21.5897 - val_acc: 0.8429\n",
      "Epoch 172/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 21.2760 - acc: 0.9153 - val_loss: 21.2296 - val_acc: 0.8678\n",
      "Epoch 173/180\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 20.9597 - acc: 0.9120 - val_loss: 20.8891 - val_acc: 0.8803\n",
      "Epoch 174/180\n",
      "38/38 [==============================] - 6s 170ms/step - loss: 20.6386 - acc: 0.9282 - val_loss: 20.5886 - val_acc: 0.8928\n",
      "Epoch 175/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 20.3723 - acc: 0.9134 - val_loss: 20.3455 - val_acc: 0.8479\n",
      "Epoch 176/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 20.0670 - acc: 0.9120 - val_loss: 19.9951 - val_acc: 0.8753\n",
      "Epoch 177/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 19.7888 - acc: 0.9065 - val_loss: 19.7256 - val_acc: 0.8853\n",
      "Epoch 178/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 19.4780 - acc: 0.9243 - val_loss: 19.4397 - val_acc: 0.8728\n",
      "Epoch 179/180\n",
      "38/38 [==============================] - 6s 169ms/step - loss: 19.2061 - acc: 0.9199 - val_loss: 19.2005 - val_acc: 0.8579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/180\n",
      "38/38 [==============================] - 6s 168ms/step - loss: 18.9501 - acc: 0.9038 - val_loss: 18.8957 - val_acc: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe599f0eba8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training code\n",
    "#%% Image data augmentation \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,               # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,                # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,    # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,     # divide each input by its std\n",
    "    zca_whitening=False,                    # apply ZCA whitening\n",
    "    rotation_range=10,                      # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.2,                  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.2,                 # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,                   # randomly flip images\n",
    "    vertical_flip=True)                     # randomly flip images\n",
    "\n",
    "datagen.fit(X_train)\n",
    "model.fit_generator(datagen.flow(X_train, y_train, shuffle=True,\n",
    "                    batch_size=32), epochs = 180,\n",
    "                     verbose= 1, validation_data = (X_test, y_test),\n",
    "                        callbacks=callbacks)   # validation_data = (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of pred test 8424\n",
      "len of id 8424\n"
     ]
    }
   ],
   "source": [
    "# from above, need to see the amount of epochs (80 +) that val stops increasing at, then do \n",
    "# that in one full go and save the model\n",
    "\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "# if not using val\n",
    "#file_path_use = filepath_full\n",
    "# if using val\n",
    "file_path_use = file_path\n",
    "inf_model = load_model(file_path_use)\n",
    "\n",
    "#score = inf_model.evaluate(X_test, y_test, verbose=1)\n",
    "#print('Test loss:', score[0])\n",
    "#print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_data[\"band_1\"]])\n",
    "X_band_test_1 = (X_band_test_1-(-22.159))/(28.424**(1/2.0))\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_data[\"band_2\"]])\n",
    "X_band_test_2 = (X_band_test_2-(-24.953))/(20.669**(1/2.0))\n",
    "X_sub = np.concatenate([X_band_test_1[:, :, :, np.newaxis]\n",
    "                          , X_band_test_2[:, :, :, np.newaxis]\n",
    "                         , ((X_band_test_1+X_band_test_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "predicted_test=inf_model.predict(X_sub)\n",
    "\n",
    "print(\"len of pred test\", len(predicted_test))\n",
    "print(\"len of id\", len(test_data['id']))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_data['id']\n",
    "submission['is_iceberg']=predicted_test\n",
    "submission.to_csv('sub_full_normed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
